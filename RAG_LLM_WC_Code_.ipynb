{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikasdatta/ML-CICD-Projects/blob/main/RAG_LLM_WC_Code_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "\n",
        "# Initialize SageMaker Session\n",
        "session = sagemaker.Session()\n",
        "role = \"arn:aws:iam::AKIAZONDITIAWMUNT46M:/role/service-role/AmazonSageMaker-ExecutionRole\"\n",
        "\n",
        "# Upload data to S3 (AWS Storage)\n",
        "# Test 1\n"
      ],
      "metadata": {
        "id": "QLRUOS_Zplmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF5QjtyU3tg1"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community chromadb sentence-transformers transformers accelerate bitsandbytes langchain_text_splitters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from transformers import pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "S7UiRDtJ3z7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data: Create a dummy text file to act as your 'knowledge base'\n",
        "with open(\"knowledge.txt\", \"w\") as f:\n",
        "    f.write(\"In 2026, the global tech industry shifted heavily towards decentralized RAG architectures.\")\n",
        "\n",
        "loader = TextLoader(\"knowledge.txt\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "oWm5P49M5fwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Chunking: Split text into manageable pieces\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "CLCVcmQQ5ni8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Embedding & Vector Store: Convert text to vectors and store them\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "pu1odcg35qWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Retrieval: Find relevant chunks based on a query\n",
        "query = \"What happened to the tech industry in 2026?\"\n",
        "docs = vectorstore.similarity_search(query, k=1)\n",
        "context = docs[0].page_content"
      ],
      "metadata": {
        "id": "uMJm0xDB5uMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Generation: Use an LLM to answer using the retrieved context\n",
        "# We use flan-t5-large as a lightweight local model for Colab\n",
        "qa_pipeline = pipeline(\"text2text-generation\",\n",
        "                       model=\"google/flan-t5-large\",\n",
        "                       device=0 if torch.cuda.is_available() else -1)"
      ],
      "metadata": {
        "id": "ayaVKUB95wod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "result = qa_pipeline(prompt, max_length=50)"
      ],
      "metadata": {
        "id": "bGR3Vs8J6Deo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Retrieved Context: {context}\")\n",
        "print(f\"LLM Response: {result[0]['generated_text']}\")"
      ],
      "metadata": {
        "id": "ajUonNqc6KW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Ask the same question without giving it any context\n",
        "base_query = \"What happened to the tech industry in 2026?\"\n",
        "\n",
        "# 2. Run the same model we loaded earlier\n",
        "# This time, the prompt is JUST the question.\n",
        "base_result = qa_pipeline(base_query, max_length=50)\n",
        "\n",
        "# 3. Compare the results\n",
        "print(f\"Base Model Response: {base_result[0]['generated_text']}\")"
      ],
      "metadata": {
        "id": "1Q4FyY_U9w7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}